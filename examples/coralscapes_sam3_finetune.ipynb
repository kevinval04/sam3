{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "# SAM 3 CoralScapes Fine-tuning Notebook\n\nThis notebook fine-tunes the SAM 3 image model for semantic segmentation on the CoralScapes dataset using the same dataloader and combined CE + Dice loss shown in the reference training snippet. Update the paths and hyper-parameters in the **Config** cell before running on your environment (e.g., Perlmutter `pscratch`).\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# Optional: install extra dependencies for notebooks/training (uncomment if needed)\n# !pip install -e \"./[notebooks,train]\"  # from the repo root\n# !pip install segmentation-models-pytorch==0.3.3  # only if you want the reference model for comparison\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "import os\nfrom pathlib import Path\nimport numpy as np\nfrom PIL import Image\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom torch.utils.data import Dataset, DataLoader\n\nimport albumentations as A\nfrom albumentations.pytorch import ToTensorV2\n\nfrom tqdm.auto import tqdm\nfrom torchmetrics.classification import Accuracy, JaccardIndex\n\nfrom sam3 import build_sam3_image_model\nfrom sam3.model.sam3_image_processor import Sam3Processor\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Config\n# ----------------------------\n# Root where your CoralScapes data lives\nroot = \"/pscratch/sd/k/kevinval/coralscapes\"  # change if different\n\nNUM_CLASSES = 40\nIGNORE_INDEX = 0\nBATCH_SIZE = 2  # adjust for your GPU memory\nNUM_EPOCHS = 10\nLR = 1e-4\n\n# Paths for SAM 3 (requires HF auth + checkpoint access)\n# If you already set HF_TOKEN and have access, the defaults work.\nbpe_path = None  # optional path to BPE file; keep None to use default shipped with the package\nsam3_checkpoint = None  # optional local checkpoint path; keep None to use the default\n\nseed = 42\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\ntorch.manual_seed(seed)\nif torch.cuda.is_available():\n    torch.cuda.manual_seed_all(seed)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Dataset (matches the reference snippet)\n# ----------------------------\nclass CoralScapesTiled(Dataset):\n    def __init__(self, root, split=\"train\", transform=None):\n        self.root = root\n        self.split = split\n        self.transform = transform\n\n        self.images_dir = os.path.join(root, split, \"images\")\n        self.masks_dir = os.path.join(root, split, \"labels\")\n\n        self.image_files = sorted([\n            f for f in os.listdir(self.images_dir)\n            if f.lower().endswith((\".png\", \".jpg\", \".jpeg\"))\n        ])\n\n        for f in self.image_files:\n            mask_path = os.path.join(self.masks_dir, f)\n            if not os.path.exists(mask_path):\n                raise FileNotFoundError(f\"Mask not found for {f}\")\n\n        print(f\"[{split}] Loaded {len(self.image_files)} tiles\")\n\n    def __len__(self):\n        return len(self.image_files)\n\n    def __getitem__(self, idx):\n        img_name = self.image_files[idx]\n\n        img_path = os.path.join(self.images_dir, img_name)\n        mask_path = os.path.join(self.masks_dir, img_name)\n\n        image = np.array(Image.open(img_path).convert(\"RGB\"))\n        mask = np.array(Image.open(mask_path))\n\n        if self.transform is not None:\n            transformed = self.transform(image=image, mask=mask)\n            image = transformed[\"image\"]\n            mask = transformed[\"mask\"]\n\n        if isinstance(mask, np.ndarray):\n            mask = torch.as_tensor(mask, dtype=torch.long)\n        else:\n            mask = mask.long()\n\n        return image, mask\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Augmentations\n# ----------------------------\n# We keep the geometric/color augmentations from the reference script.\n# The normalization matches the Sam3Processor defaults (mean=0.5, std=0.5).\ntrain_transform = A.Compose([\n    A.HorizontalFlip(p=0.5),\n    A.VerticalFlip(p=0.2),\n    A.RandomRotate90(p=0.5),\n    A.ShiftScaleRotate(\n        shift_limit=0.05,\n        scale_limit=0.1,\n        rotate_limit=10,\n        border_mode=0,\n        p=0.5,\n    ),\n    A.ColorJitter(0.2, 0.2, 0.2, 0.1, p=0.5),\n    A.GaussianBlur(blur_limit=3, p=0.2),\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2(),\n])\n\nval_transform = A.Compose([\n    A.Normalize(mean=(0.5, 0.5, 0.5), std=(0.5, 0.5, 0.5)),\n    ToTensorV2(),\n])\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Datasets & DataLoaders\n# ----------------------------\ntrain_ds = CoralScapesTiled(root, split=\"train\", transform=train_transform)\nval_ds = CoralScapesTiled(root, split=\"validation\", transform=val_transform)\n\ndef seed_worker(worker_id):\n    np.random.seed(seed + worker_id)\n    torch.manual_seed(seed + worker_id)\n\ntrain_loader = DataLoader(\n    train_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=True,\n    num_workers=4,\n    pin_memory=True,\n    worker_init_fn=seed_worker,\n)\n\nval_loader = DataLoader(\n    val_ds,\n    batch_size=BATCH_SIZE,\n    shuffle=False,\n    num_workers=4,\n    pin_memory=True,\n    worker_init_fn=seed_worker,\n)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Build SAM 3 model + lightweight segmentation head\n# ----------------------------\n# We reuse SAM 3 as a strong visual backbone and add a per-pixel classifier on top\n# of the highest-resolution FPN feature map. The entire module is trainable so you\n# can choose to freeze the backbone if desired.\n\nsam3_model = build_sam3_image_model(\n    bpe_path=bpe_path,\n    ckpt_path=sam3_checkpoint,\n).to(device)\nprocessor = Sam3Processor(sam3_model, device=device)\n\nclass Sam3SemanticSegmentation(nn.Module):\n    def __init__(self, sam3_model, num_classes):\n        super().__init__()\n        self.sam3 = sam3_model\n        self.classifier = nn.Sequential(\n            nn.Conv2d(256, 256, kernel_size=3, padding=1),\n            nn.GroupNorm(8, 256),\n            nn.ReLU(inplace=True),\n            nn.Conv2d(256, num_classes, kernel_size=1),\n        )\n\n    def forward(self, images):\n        # images are already normalized tensors (B, 3, H, W)\n        backbone_out = self.sam3.backbone.forward_image(images)\n        fpn_feats = backbone_out[\"backbone_fpn\"]  # list of multi-scale features\n        # use the highest-resolution feature map (last in the list)\n        feat = fpn_feats[-1]\n        logits = self.classifier(feat)\n        # upsample to input resolution\n        logits = F.interpolate(logits, size=images.shape[-2:], mode=\"bilinear\", align_corners=False)\n        return logits\n\nmodel = Sam3SemanticSegmentation(sam3_model, NUM_CLASSES).to(device)\n\n# Uncomment the next line if you want to freeze the SAM 3 backbone and only train the head\n# for p in model.sam3.parameters():\n#     p.requires_grad = False\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Loss: CE + Dice (same structure as the reference)\n# ----------------------------\nclass CombinedCELossDiceLoss(nn.Module):\n    def __init__(self, weight_ce=0.5, weight_dice=0.5, ignore_index=0, eps=1e-6):\n        super().__init__()\n        self.weight_ce = weight_ce\n        self.weight_dice = weight_dice\n        self.ignore_index = ignore_index\n        self.eps = eps\n        self.ce_loss = nn.CrossEntropyLoss(ignore_index=ignore_index)\n\n    def forward(self, logits, targets):\n        ce = self.ce_loss(logits, targets)\n\n        num_classes = logits.shape[1]\n        targets_one_hot = F.one_hot(targets.clamp(min=0), num_classes=num_classes)\n        targets_one_hot = targets_one_hot.permute(0, 3, 1, 2).float()\n\n        probs = F.softmax(logits, dim=1)\n        valid_mask = (targets != self.ignore_index).unsqueeze(1).float()\n        probs = probs * valid_mask\n        targets_one_hot = targets_one_hot * valid_mask\n\n        intersection = (probs * targets_one_hot).sum(dim=(0, 2, 3))\n        union = probs.sum(dim=(0, 2, 3)) + targets_one_hot.sum(dim=(0, 2, 3))\n        dice_loss = 1.0 - ((2.0 * intersection + self.eps) / (union + self.eps)).mean()\n\n        return self.weight_ce * ce + self.weight_dice * dice_loss\n\nloss_fn = CombinedCELossDiceLoss(weight_ce=0.3, weight_dice=0.7, ignore_index=IGNORE_INDEX)\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Optimizer, Scheduler, Metrics\n# ----------------------------\noptimizer = torch.optim.AdamW(model.parameters(), lr=LR)\nscheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=NUM_EPOCHS)\n\nscaler = torch.cuda.amp.GradScaler(enabled=(device == \"cuda\"))\n\naccuracy_metric = Accuracy(\n    task=\"multiclass\",\n    num_classes=NUM_CLASSES,\n    ignore_index=IGNORE_INDEX,\n).to(device)\n\nmiou_metric = JaccardIndex(\n    task=\"multiclass\",\n    num_classes=NUM_CLASSES,\n    ignore_index=IGNORE_INDEX,\n).to(device)\n\nbest_miou = 0.0\n"
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": "# ----------------------------\n# Training Loop\n# ----------------------------\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    train_bar = tqdm(train_loader, desc=f\"Epoch {epoch+1}/{NUM_EPOCHS}\")\n\n    running_loss = 0.0\n\n    for images, masks in train_bar:\n        images = images.to(device)\n        masks = masks.to(device).long()\n\n        optimizer.zero_grad(set_to_none=True)\n\n        with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n            logits = model(images)\n            loss = loss_fn(logits, masks)\n\n        scaler.scale(loss).backward()\n        scaler.step(optimizer)\n        scaler.update()\n\n        running_loss += loss.item() * images.size(0)\n        avg_loss = running_loss / ((train_bar.n + 1e-9) * BATCH_SIZE)\n        train_bar.set_postfix(loss=avg_loss)\n\n    scheduler.step()\n\n    # ------------------------\n    # Validation\n    # ------------------------\n    model.eval()\n    with torch.no_grad():\n        for images, masks in val_loader:\n            images = images.to(device)\n            masks = masks.to(device).long()\n\n            with torch.cuda.amp.autocast(enabled=(device == \"cuda\")):\n                logits = model(images)\n\n            preds = logits.argmax(dim=1)\n\n            accuracy_metric.update(preds.view(-1), masks.view(-1))\n            miou_metric.update(preds, masks)\n\n        val_accuracy = accuracy_metric.compute().item()\n        val_miou = miou_metric.compute().item()\n\n        print(f\"\\nEpoch {epoch+1}/{NUM_EPOCHS} | Val Accuracy: {val_accuracy:.4f} | Val mIoU: {val_miou:.4f}\")\n\n        accuracy_metric.reset()\n        miou_metric.reset()\n\n        if val_miou > best_miou:\n            best_miou = val_miou\n            torch.save(model.state_dict(), \"best_sam3_coralscapes_semantic.pth\")\n            print(f\"  -> Saved best checkpoint with mIoU: {best_miou:.4f}\\n\")\n"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}